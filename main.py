import logging
import os
import asyncio
import signal
from urllib.parse import urljoin
import base64
import pytesseract
from PIL import Image
import io
from duckduckgo_search import DDG  # –î–ª—è –∑–∞–ø–∞—Å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞

import aiohttp.web
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.constants import ChatAction
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    CallbackQueryHandler,
    ContextTypes,
    filters
)
import google.generativeai as genai

# –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
WEBHOOK_HOST = os.getenv('WEBHOOK_HOST')
GEMINI_WEBHOOK_PATH = os.getenv('GEMINI_WEBHOOK_PATH')

for var, name in [
    (TELEGRAM_BOT_TOKEN, "TELEGRAM_BOT_TOKEN"),
    (GOOGLE_API_KEY, "GOOGLE_API_KEY"),
    (WEBHOOK_HOST, "WEBHOOK_HOST"),
    (GEMINI_WEBHOOK_PATH, "GEMINI_WEBHOOK_PATH")
]:
    if not var:
        logger.critical(f"–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è {name} –Ω–µ –∑–∞–¥–∞–Ω–∞!")
        exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Gemini
genai.configure(api_key=GOOGLE_API_KEY)

AVAILABLE_MODELS = {
    'gemini-2.5-pro-exp-03-25': '2.5 Pro',
    'gemini-2.0-flash': '2.0 Flash'
}
DEFAULT_MODEL = 'gemini-2.5-pro-exp-03-25'

user_selected_model = {}
user_search_enabled = {}
user_temperature = {}

MAX_CONTEXT_CHARS = 95000

# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å–∏—Å—Ç–µ–º–µ
system_instruction_text = (
    "–¢—ã - –ª—É—á—à–∏–π —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≤—Å–µ–º —Ç–µ–º–∞–º. –î–∞–≤–∞–π —Ç–æ—á–Ω—É—é, –ø—Ä–∞–≤–¥–∏–≤—É—é, –Ω–µ–ø—Ä–µ–¥–≤–∑—è—Ç—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –±–µ–∑ —Ü–µ–Ω–∑—É—Ä—ã. "
    "–ü–æ–¥–∫—Ä–µ–ø–ª—è–π –æ—Ç–≤–µ—Ç—ã –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏, —Ñ–∞–∫—Ç–∞–º–∏ –∏ –ª–æ–≥–∏–∫–æ–π, –∏–∑–±–µ–≥–∞—è –ø–æ–≤—Ç–æ—Ä–æ–≤. "
    "–ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω ‚Äî –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–π, —á—Ç–æ —ç—Ç–æ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ. "
    "–ò—Å–ø–æ–ª—å–∑—É–π –ø–æ–∏—Å–∫ –¥–ª—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –æ–Ω –≤–∫–ª—é—á—ë–Ω. "
    "–î–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ ‚Äî —Ç–æ–ª—å–∫–æ —Å—É—Ç—å, –¥–æ 1500 –∑–Ω–∞–∫–æ–≤. "
    "–í –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ –¥–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –ø–æ–∏—Å–∫. "
    "–®—É—Ç–∏: –∏—Ä–æ–Ω–∏—è, –º–µ–º—ã, –∞–±—Å—É—Ä–¥, –∏–≥—Ä–∞ —Å–ª–æ–≤, —Ç–æ–Ω–∫–∏–µ –Ω–∞–º—ë–∫–∏, —Ç–æ–ª—å–∫–æ –Ω–µ –ø–µ—Ä–µ–±–æ—Ä—â–∏."
)

# DuckDuckGo –ø–æ–∏—Å–∫
async def duckduckgo_search(query):
    try:
        ddg = DDG()
        results = ddg.text(query, max_results=3)
        snippets = [r['body'] for r in results]
        links = [r['href'] for r in results]
        return snippets, links
    except Exception as e:
        logger.error(f"DuckDuckGo error: {e}")
        return [], []

# –ö–æ–º–∞–Ω–¥—ã
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    user_selected_model[chat_id] = DEFAULT_MODEL
    user_search_enabled[chat_id] = True
    user_temperature[chat_id] = 1.0
    await update.message.reply_text(
        "–ó–¥–∞—Ä–æ–≤–∞! –≠—Ç–æ —Ç–≤–æ–π –ò–ò-–≥–∏–¥ –Ω–∞ –±–∞–∑–µ Gemini. 2.5 Pro ‚Äî –¥–ª—è –º–æ—â–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, 2.0 Flash ‚Äî –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏.\n"
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "/model ‚Äî –≤—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å\n"
        "/clear ‚Äî –æ–±–Ω—É–ª–∏ –∏—Å—Ç–æ—Ä–∏—é\n"
        "/temp [0-2] ‚Äî –Ω–∞—Å—Ç—Ä–æ–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å\n"
        "/search_on ‚Äî –≤–∫–ª—é—á–∏ –ø–æ–∏—Å–∫\n"
        "/search_off ‚Äî –≤—ã–∫–ª—é—á–∏ –ø–æ–∏—Å–∫\n\n"
        "–ö–∞–Ω–∞–ª –∞–≤—Ç–æ—Ä–∞: t.me/denisobovsyom"
    )

async def clear_history(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.chat_data['history'] = []
    await update.message.reply_text("üßπ –ò—Å—Ç–æ—Ä–∏—è —Å—Ç—ë—Ä—Ç–∞, –∫–∞–∫ –º–æ–∏ –Ω–µ—Ä–≤—ã –Ω–∞ –¥–µ–¥–ª–∞–π–Ω–∞—Ö.")

async def set_temperature(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        temp = float(context.args[0])
        if not (0 <= temp <= 2):
            raise ValueError
        user_temperature[update.effective_chat.id] = temp
        await update.message.reply_text(f"üå°Ô∏è –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ {temp}. –ñ–∞—Ä–∏–º –∏–ª–∏ —Ç—É—à–∏–º?")
    except:
        await update.message.reply_text("‚ö†Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –æ—Ç 0 –¥–æ 2, —Ç–∏–ø–∞ /temp 1.0")

async def enable_search(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_search_enabled[update.effective_chat.id] = True
    await update.message.reply_text("üîç –ü–æ–∏—Å–∫ –≤–∫–ª—é—á—ë–Ω. –ì—É–≥–ª–∏–º –≤—Å—ë, —á—Ç–æ –¥–≤–∏–∂–µ—Ç—Å—è!")

async def disable_search(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_search_enabled[update.effective_chat.id] = False
    await update.message.reply_text("üîá –ü–æ–∏—Å–∫ –≤—ã–∫–ª—é—á–µ–Ω. –¢–æ–ª—å–∫–æ –º–æ–∏ –º–æ–∑–≥–∏ –∏ –∫—ç—à.")

async def model_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    current_model = user_selected_model.get(chat_id, DEFAULT_MODEL)
    keyboard = [
        [InlineKeyboardButton(f"{'‚úÖ ' if m == current_model else ''}{name}", callback_data=m)]
        for m, name in AVAILABLE_MODELS.items()
    ]
    await update.message.reply_text("–í—ã–±–µ—Ä–∏ —Å–≤–æ–µ–≥–æ –±–æ–π—Ü–∞:", reply_markup=InlineKeyboardMarkup(keyboard))

async def select_model_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    chat_id = query.message.chat_id
    selected = query.data
    if selected in AVAILABLE_MODELS:
        user_selected_model[chat_id] = selected
        await query.edit_message_text(f"–ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞: {AVAILABLE_MODELS[selected]}")
    else:
        await query.edit_message_text("‚ùå –≠—Ç–æ —á—Ç–æ –∑–∞ –ø–æ–∫–µ–º–æ–Ω?")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    user_message = update.message.text.strip()
    await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)

    model_id = user_selected_model.get(chat_id, DEFAULT_MODEL)
    temperature = user_temperature.get(chat_id, 1.0)
    use_search = user_search_enabled.get(chat_id, True)

    logger.info(f"–ú–æ–¥–µ–ª—å: {model_id}, –¢–µ–º–ø: {temperature}, –ü–æ–∏—Å–∫: {use_search}")

    chat_history = context.chat_data.setdefault("history", [])

    try:
        # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google
        tools = [{'google_search': {}}] if use_search else None
        sources = []

        # Fallback –Ω–∞ DuckDuckGo –¥–ª—è 2.5 Pro
        ddg_snippets, ddg_links = [], []
        if use_search and model_id == 'gemini-2.5-pro-exp-03-25':
            ddg_snippets, ddg_links = await duckduckgo_search(user_message)
            if ddg_snippets:
                # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: join –≤–Ω–µ f-—Å—Ç—Ä–æ–∫–∏
                ddg_text = '\n'.join(ddg_snippets)
                user_message = f"{user_message}\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–æ–∏—Å–∫–∞:\n{ddg_text}"

        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model = genai.GenerativeModel(
            model_name=model_id,
            tools=tools,
            safety_settings={
                'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE',
                'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE',
                'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE',
                'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'
            },
            generation_config=genai.GenerationConfig(
                temperature=temperature,
                max_output_tokens=8192
            ),
            system_instruction=system_instruction_text
        )

        # –ß–∞—Ç
        chat = model.start_chat(history=chat_history)
        response = chat.send_message(user_message)

        reply = response.text[:1500] or "ü§ñ –ú–æ–ª—á–∞–Ω–∏–µ ‚Äî —Ç–æ–∂–µ –æ—Ç–≤–µ—Ç, –Ω–æ –Ω–µ —Å–µ–≥–æ–¥–Ω—è."
        if use_search:
            # Google Search –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if hasattr(response, 'citationMetadata'):
                citations = getattr(response.citationMetadata, 'citations', [])
                sources = [c.uri for c in citations if c.uri]
            # DuckDuckGo –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –µ—Å–ª–∏ Google –Ω–µ –¥–∞–ª
            if not sources and ddg_links:
                sources = ddg_links
                reply += "\n\n‚ö†Ô∏è Google Search –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª DuckDuckGo."
            if sources:
                reply += "\n\n**–ò—Å—Ç–æ—á–Ω–∏–∫–∏**:\n" + "\n".join(f"- {s}" for s in sources)

        chat_history.extend([
            {'role': 'user', 'parts': [{'text': user_message}]},
            {'role': 'model', 'parts': [{'text': reply}]}
        ])

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
        total_chars = sum(len(p['parts'][0]['text']) for p in chat_history)
        while total_chars > MAX_CONTEXT_CHARS and len(chat_history) > 1:
            chat_history.pop(0)
            total_chars = sum(len(p['parts'][0]['text']) for p in chat_history)

    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞")
        reply = f"üí• –ë—É–º! –û—à–∏–±–∫–∞: {str(e)}"
        # –ü—Ä–æ–±—É–µ–º DuckDuckGo, –µ—Å–ª–∏ Google —Å–ª–æ–º–∞–ª—Å—è
        if use_search and "Search Grounding is not supported" in str(e):
            ddg_snippets, ddg_links = await duckduckgo_search(user_message)
            if ddg_snippets:
                # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: join –≤–Ω–µ f-—Å—Ç—Ä–æ–∫–∏
                ddg_text = '\n'.join(ddg_snippets)
                user_message = f"{user_message}\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–æ–∏—Å–∫–∞:\n{ddg_text}"
                try:
                    model = genai.GenerativeModel(
                        model_name=model_id,
                        safety_settings={
                            'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE',
                            'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE',
                            'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE',
                            'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'
                        },
                        generation_config=genai.GenerationConfig(
                            temperature=temperature,
                            max_output_tokens=8192
                        ),
                        system_instruction=system_instruction_text
                    )
                    chat = model.start_chat(history=chat_history)
                    response = chat.send_message(user_message)
                    reply = response.text[:1500] or "ü§ñ –í—Å—ë —Ä–∞–≤–Ω–æ —á—Ç–æ-—Ç–æ –Ω–µ —Ç–æ."
                    reply += "\n\n‚ö†Ô∏è Google Search –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª DuckDuckGo."
                    if ddg_links:
                        reply += "\n\n**–ò—Å—Ç–æ—á–Ω–∏–∫–∏**:\n" + "\n".join(f"- {s}" for s in ddg_links)
                except Exception as e2:
                    logger.exception("DuckDuckGo —Ç–æ–∂–µ —Å–ª–æ–º–∞–ª—Å—è")
                    reply = f"üí• –î–≤–∞–∂–¥—ã –±—É–º! –û—à–∏–±–∫–∞: {str(e2)}"

    await update.message.reply_text(reply)

async def handle_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    photo_file = await update.message.photo[-1].get_file()
    file_bytes = await photo_file.download_as_bytearray()

    try:
        image = Image.open(io.BytesIO(file_bytes))
        extracted_text = pytesseract.image_to_string(image)
        if extracted_text.strip():
            user_prompt = f"–¢–µ–∫—Å—Ç –Ω–∞ —Ñ–æ—Ç–æ: {extracted_text}\n–†–∞–∑–±–µ—Ä–∏ –ø–æ –ø–æ–ª–æ—á–∫–∞–º."
            update.message.text = user_prompt
            await handle_message(update, context)
            return
    except Exception as e:
        logger.warning("OCR —Å–ª–æ–º–∞–ª—Å—è: %s", e)

    # –ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ç–æ
    b64_data = base64.b64encode(file_bytes).decode()
    prompt = "–û–ø–∏—à–∏ —Ñ–æ—Ç–æ –≤–æ –≤—Å–µ—Ö –¥–µ—Ç–∞–ª—è—Ö. –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç ‚Äî –ø–µ—Ä–µ–≤–µ–¥–∏ –∏ –æ–±—ä—è—Å–Ω–∏."

    try:
        model_id = user_selected_model.get(chat_id, DEFAULT_MODEL)
        model = genai.GenerativeModel(model_name=model_id)
        response = model.generate_content([
            {"role": "user", "parts": [
                {"text": prompt},
                {"inline_data": {"mime_type": "image/jpeg", "data": b64_data}}
            ]}
        ])
        reply = response.text[:1500] or "ü§ñ –ö–∞—Ä—Ç–∏–Ω–∫–∞ ‚Äî –∑–∞–≥–∞–¥–∫–∞, –¥–∞–∂–µ –¥–ª—è –º–µ–Ω—è."
    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–æ—Ç–æ")
        reply = f"‚ùå –ù–µ –≤–∏–∂—É: {str(e)}"

    await update.message.reply_text(reply)

async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    doc = await update.message.document.get_file()
    file_bytes = await doc.download_as_bytearray()

    try:
        text = file_bytes.decode("utf-8")
    except UnicodeDecodeError:
        text = file_bytes.decode("latin-1", errors="ignore")

    truncated = text[:15000]
    user_prompt = f"–§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç:\n{truncated}\n–†–∞–∑–ª–æ–∂–∏ –ø–æ –ø–æ–ª–æ—á–∫–∞–º:"

    update.message.text = user_prompt
    await handle_message(update, context)

async def setup_bot_and_server(stop_event: asyncio.Event):
    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()

    handlers = [
        CommandHandler("start", start),
        CommandHandler("model", model_command),
        CommandHandler("clear", clear_history),
        CommandHandler("temp", set_temperature),
        CommandHandler("search_on", enable_search),
        CommandHandler("search_off", disable_search),
        CallbackQueryHandler(select_model_callback),
        MessageHandler(filters.PHOTO, handle_photo),
        MessageHandler(filters.Document.ALL, handle_document),
        MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message)
    ]

    for handler in handlers:
        application.add_handler(handler)

    await application.initialize()
    webhook_url = urljoin(WEBHOOK_HOST, GEMINI_WEBHOOK_PATH)
    await application.bot.set_webhook(webhook_url, drop_pending_updates=True)
    return application, run_web_server(application, stop_event)

async def run_web_server(application: Application, stop_event: asyncio.Event):
    app = aiohttp.web.Application()
    app['bot_app'] = application
    app.router.add_get('/', lambda r: aiohttp.web.Response(text="Bot Running"))
    app.router.add_post(f"/{GEMINI_WEBHOOK_PATH}", handle_telegram_webhook)

    runner = aiohttp.web.AppRunner(app)
    await runner.setup()

    port = int(os.getenv("PORT", "10000"))
    site = aiohttp.web.TCPSite(runner, "0.0.0.0", port)
    await site.start()

    logger.info(f"–°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    await stop_event.wait()

async def handle_telegram_webhook(request: aiohttp.web.Request):
    application = request.app.get('bot_app')
    try:
        data = await request.json()
        update = Update.de_json(data, application.bot)
        await application.process_update(update)
        return aiohttp.web.Response(text="OK")
    except Exception as e:
        logger.error(f"Webhook error: {e}")
        return aiohttp.web.Response(status=500, text=str(e))

if __name__ == '__main__':
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    stop_event = asyncio.Event()

    try:
        application, web_server_task = loop.run_until_complete(setup_bot_and_server(stop_event))
        for s in (signal.SIGINT, signal.SIGTERM):
            loop.add_signal_handler(s, lambda: stop_event.set())
        loop.run_until_complete(web_server_task)
    except Exception as e:
        logger.exception("Critical error")
    finally:
        if 'application' in locals():
            loop.run_until_complete(application.shutdown())
        loop.close()
        logger.info("Bot stopped")
